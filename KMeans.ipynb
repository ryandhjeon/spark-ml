{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "instant-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.mllib.regression import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "entitled-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"KMeans\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "saved-bookmark",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.textFile(\"./dataset/Qualitative_Bankruptcy.txt\")\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-baptist",
   "metadata": {},
   "source": [
    "### Prepare data for the logistic regression algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opponent-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDoubleValue(input):\n",
    "    result = 0.0\n",
    "    if (input == 'P'):\n",
    "        result = 3.0\n",
    "    if (input == 'A'): \n",
    "        result = 2.0\n",
    "    if (input == 'N'): \n",
    "        result = 1.0\n",
    "    if (input == 'NB'): \n",
    "        result = 1.0        \n",
    "    if (input == 'B'): \n",
    "        result = 0.0      \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "designed-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTuple = data.map(lambda o: (getDoubleValue(o.split(\",\")[0]),getDoubleValue(o.split(\",\")[1]),getDoubleValue(o.split(\",\")[2]),getDoubleValue(o.split(\",\")[3]),getDoubleValue(o.split(\",\")[4]),getDoubleValue(o.split(\",\")[5]),getDoubleValue(o.split(\",\")[6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "comprehensive-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataTuple.toDF(['Industrial Risk','Management Risk','Financial Flexibility','Credibility','Competitiveness','Operating Risk','Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "worse-words",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+---------------------+-----------+---------------+--------------+-----+\n",
      "|Industrial Risk|Management Risk|Financial Flexibility|Credibility|Competitiveness|Operating Risk|Class|\n",
      "+---------------+---------------+---------------------+-----------+---------------+--------------+-----+\n",
      "|            3.0|            3.0|                  2.0|        2.0|            2.0|           3.0|  1.0|\n",
      "|            1.0|            1.0|                  2.0|        2.0|            2.0|           1.0|  1.0|\n",
      "|            2.0|            2.0|                  2.0|        2.0|            2.0|           2.0|  1.0|\n",
      "|            3.0|            3.0|                  3.0|        3.0|            3.0|           3.0|  1.0|\n",
      "|            1.0|            1.0|                  3.0|        3.0|            3.0|           1.0|  1.0|\n",
      "+---------------+---------------+---------------------+-----------+---------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "arbitrary-carolina",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Industrial Risk: double (nullable = true)\n",
      " |-- Management Risk: double (nullable = true)\n",
      " |-- Financial Flexibility: double (nullable = true)\n",
      " |-- Credibility: double (nullable = true)\n",
      " |-- Competitiveness: double (nullable = true)\n",
      " |-- Operating Risk: double (nullable = true)\n",
      " |-- Class: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "employed-cancellation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|  0.0|  107|\n",
      "|  1.0|  143|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('Class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rural-heating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|features                 |\n",
      "+-------------------------+\n",
      "|[3.0,3.0,2.0,2.0,2.0,3.0]|\n",
      "|[1.0,1.0,2.0,2.0,2.0,1.0]|\n",
      "|[2.0,2.0,2.0,2.0,2.0,2.0]|\n",
      "|[3.0,3.0,3.0,3.0,3.0,3.0]|\n",
      "|[1.0,1.0,3.0,3.0,3.0,1.0]|\n",
      "|[2.0,2.0,3.0,3.0,3.0,2.0]|\n",
      "|[3.0,3.0,2.0,3.0,3.0,3.0]|\n",
      "|[3.0,3.0,3.0,2.0,2.0,3.0]|\n",
      "|[3.0,3.0,2.0,3.0,2.0,3.0]|\n",
      "|[3.0,3.0,2.0,2.0,3.0,3.0]|\n",
      "|[3.0,3.0,3.0,3.0,2.0,3.0]|\n",
      "|[3.0,3.0,3.0,2.0,3.0,3.0]|\n",
      "|[1.0,1.0,2.0,3.0,3.0,1.0]|\n",
      "|[1.0,1.0,3.0,2.0,2.0,1.0]|\n",
      "|[1.0,1.0,2.0,3.0,2.0,1.0]|\n",
      "|[1.0,1.0,2.0,3.0,2.0,1.0]|\n",
      "|[1.0,1.0,2.0,2.0,3.0,1.0]|\n",
      "|[1.0,1.0,3.0,3.0,2.0,1.0]|\n",
      "|[1.0,1.0,3.0,2.0,3.0,1.0]|\n",
      "|[2.0,2.0,2.0,3.0,3.0,2.0]|\n",
      "+-------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ignore = ['Class']\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[x for x in df.columns if x not in ignore],\n",
    "    outputCol='features')\n",
    "\n",
    "assembler_df = assembler.transform(df)\n",
    "assembler_df.select(\"features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "resistant-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_df = assembler_df.withColumn(\"label\", assembler_df[\"Class\"])\n",
    "# label_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-turtle",
   "metadata": {},
   "source": [
    "### Standard scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "simple-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().setInputCol('features').setOutputCol('scaled_features')\n",
    "\n",
    "scaler_model = scaler.fit(assembler_df)\n",
    "scaler_df = scaler_model.transform(assembler_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "middle-deviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+---------------------+-----------+---------------+--------------+-----+--------------------+--------------------+\n",
      "|Industrial Risk|Management Risk|Financial Flexibility|Credibility|Competitiveness|Operating Risk|Class|            features|     scaled_features|\n",
      "+---------------+---------------+---------------------+-----------+---------------+--------------+-----+--------------------+--------------------+\n",
      "|            3.0|            3.0|                  2.0|        2.0|            2.0|           3.0|  1.0|[3.0,3.0,2.0,2.0,...|[3.64497271330941...|\n",
      "|            1.0|            1.0|                  2.0|        2.0|            2.0|           1.0|  1.0|[1.0,1.0,2.0,2.0,...|[1.21499090443647...|\n",
      "|            2.0|            2.0|                  2.0|        2.0|            2.0|           2.0|  1.0|[2.0,2.0,2.0,2.0,...|[2.42998180887294...|\n",
      "|            3.0|            3.0|                  3.0|        3.0|            3.0|           3.0|  1.0|[3.0,3.0,3.0,3.0,...|[3.64497271330941...|\n",
      "|            1.0|            1.0|                  3.0|        3.0|            3.0|           1.0|  1.0|[1.0,1.0,3.0,3.0,...|[1.21499090443647...|\n",
      "+---------------+---------------+---------------------+-----------+---------------+--------------+-----+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "introductory-cornell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_data = scaler_df.rdd.map(lambda x: LabeledPoint(x[6], x[:6]))\n",
    "# parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "digital-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "final-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(parsed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-receipt",
   "metadata": {},
   "source": [
    "### Split train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "turned-arrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count:152\n",
      "Test Dataset Count:98\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = scaler_df.randomSplit([0.6, 0.4], seed=11)\n",
    "\n",
    "print(\"Training Dataset Count:\" + str(train_data.count()))\n",
    "print(\"Test Dataset Count:\" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-department",
   "metadata": {},
   "source": [
    "### KMeans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "accomplished-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol='scaled_features', k=2)\n",
    "model = kmeans.fit(train_data)\n",
    "output = model.transform(scaler_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "embedded-circulation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster centroids: \n",
      "[2.04812752 1.634819   1.33400568 1.49496302 1.26427356 1.85731548]\n",
      "[2.60778536 2.59814903 2.83937144 3.02177836 2.96104096 2.42737864]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 44574)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "centroids = model.clusterCenters()\n",
    "print(\"Cluster centroids: \")\n",
    "for centroid in centroids:\n",
    "    print(centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "revised-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score=[]\n",
    "evaluator = ClusteringEvaluator(predictionCol='prediction', featuresCol='scaled_features', \\\n",
    "                                metricName='silhouette', distanceMeasure='squaredEuclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "maritime-business",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.5181090713027618\n"
     ]
    }
   ],
   "source": [
    "score=evaluator.evaluate(output)    \n",
    "silhouette_score.append(score)\n",
    "print(\"Silhouette Score:\",score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-norway",
   "metadata": {},
   "source": [
    "__Silhouette score of 0.5 is not a very good score, meaning the data was not clustered completely.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-robert",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
